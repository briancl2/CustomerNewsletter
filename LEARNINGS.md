# Learnings

> Every run produces at least one lesson that makes the next run better.
> Every lesson has an ID, evidence, and a concrete fix.

| ID | Lesson | Evidence | Fix |
|---|---|---|---|
| L1 | Analysis without action is the default failure mode | autonomous_loops_guide.md principle #1 | Enforce ≤40% analysis, ≥60% execution in all phases |
| L2 | Trust disk, not self-reports | autonomous_loops_guide.md principle #2; agents claim completion without artifacts | Every phase gate checks files with ls and wc -c |
| L3 | Run cheapest tests first | autonomous_loops_guide.md scoring stack | Layer scoring: structural (free) → heuristic (5s) → LLM-judge (5min) |
| L4 | One fix per iteration | autonomous_loops_guide.md principle #4 | Rework loop changes one file, re-measures, decides |
| L5 | Stop rules prevent runaway | autonomous_loops_guide.md principle #10 | Max 3 rework cycles per skill, 5 total per run, 600s timeout per CLI session |
| L6 | grep -P (Perl regex) not available on macOS BSD grep | score-heuristic.sh failed silently with grep -oP | Use grep -E for extended regex; avoid -P entirely |
| L7 | set -o pipefail kills grep chains that find nothing | score-heuristic.sh exited on grep pipe returning 1 | Wrap grep in (grep ... \|\| true) before piping to wc |
| L8 | Forbidden pattern mentions in rules ("never use X") trigger pattern scanners | Heuristic T1.3 finds "Copilot Free" in exclusion rules | Accept as known false positive; scorer still passes tier thresholds |
| L9 | Direct skill building beats fleet dispatch for first run | Built 8 skills in minutes with full context vs 30-60 min fleet latency | Use fleet for subsequent runs; use direct for initial build |
| L10 | Agent refactor is straightforward once skills exist | 454->81 lines in one pass; all domain logic cleanly extractable | Skills-first extraction works; keep agent as pure orchestrator |
| L11 | Governance items get 3x weight in actual editorial practice | 16% of published bullets are governance + they always get expanded treatment | Update selection-criteria.md with 3.0x governance weight |
| L12 | Lead sections have 4 distinct triggers, not just item clustering | Dec=governance cluster, Jun=blockbuster launch, Jan=hot topic, Dec2024=strategic positioning | Encode ALL 4 triggers in theme detection rules, not just clustering |
| L13 | Model availability is always commodity-compressed | 100% of model updates across Dec and Aug are single-bullet consolidated | Make model compression a hard rule, not a suggestion |
| L14 | 7 editorial judgment calls exist that no current rule captures | Novelty scoring, product category detection, ecosystem maturity, audience priority override, magnitude recategorization, structural conventions, evergreen inclusion | These are the real blind spots; address via human Q&A and progressive rule encoding |
| L15 | 14 published newsletters + 4 rich intermediate cycles = sufficient data for editorial pattern extraction | 549 lines of findings from 6 parallel analyses | The benchmark data is an underused asset; structural analysis alone reveals clear patterns |
| L16 | The human's audience-specific emphasis (Healthcare/Manufacturing/FinServ) creates a governance+security bias that generic enterprise filters miss | BYOK, indemnity, data residency consistently prioritized | Add audience-specific weight boosts for regulated-industry signals |
| L17 | Cross-cycle patterns are either structural constants or conditional triggers, never random | Introduction, Copilot, Scale, Events, Closing = always. Lead section, IDE parity, conference tables = conditional with clear triggers | Encode stable patterns as mandatory; conditional patterns as triggered |
| L18 | Competitive positioning is the highest-weight editorial signal | Feb 2026: Agent HQ 3P (Claude+Codex) was THE story; CLI and OpenCode are counter-Claude-Code signals | Add competitive positioning as 3.5x weight, highest in the model |
| L19 | GitHub Blog news-insights posts are NOT in the changelog feed | Agent HQ 3P post was at github.blog/news-insights/company-news/, invisible to changelog-only scanning | url-manifest MUST include blog homepage scan and news-insights/ |
| L20 | VS Code changelogs hide important features behind terse entries | Human says "VS Code updates usually has really important functionality hidden in the change log, please look deeper" | Phase 1B must deep-read the actual VS Code release notes page, not just the changelog entry |
| L21 | Bundling related enterprise items into single bullets is preferred over separate entries | Human bundled supply chain + org custom props + dependabot OIDC into one governance bullet | Add bundling heuristic: when 3+ related governance/security items exist, consolidate into one rich bullet |
| L22 | The human sees platform-openness as a theme (BYOK + 3P + OpenCode + CLI = "take your subscription anywhere") | Feb 2026 correction: the narrative is about customer choice, not just features | Add "platform openness" theme detection: when BYOK + multi-surface + 3P agent support cluster, lead with choice narrative |
| L23 | The editorial review loop must be automated, not manual. Human should not have to ask for iteration. | "i dont want to go through this process manually where i give you the corrections and have to tell you to iterate" | Build review-correct-regenerate into the core pipeline as Phase 5 |
| L24 | Visual Studio release notes are a ROLLING page of patches/CVEs, not versioned feature pages | VS 2022 17.14 has 26 patch versions (17.14.1-17.14.26) on one page. Features section is undated. Patch notes are bug fixes/CVEs. | For VS feature content, use GitHub Changelog ("Copilot in Visual Studio - [Month] update") as primary source. MS Learn page is secondary for patch timeline and buried Copilot items. |
| L25 | Each source URL has a fundamentally different data model requiring source-specific extraction logic | VS Code: versioned pages with rich content. VS: rolling patch page + GH Changelog cross-ref. JetBrains: API with terse notes. Xcode: single CHANGELOG.md. GH Changelog: monthly archives, richest source. | Build per-source intelligence in reference/source-intelligence/{source}.md, not one-size-fits-all extraction. |
| L26 | Per-source item-level tracing (raw to intermediate to published) is the highest-signal training data | Knowing that "VS Code v1.106 Agent Mode Approval" became part of "Copilot Agents Expansion" bullet teaches the consolidation pattern better than any rule description. | Build source-specific translation intelligence from the 4 rich benchmark cycles. |
| L27 | The content-retrieval skill (Phase 1B) and content-consolidation skill (Phase 1C) need source-specific extraction strategies, not a universal format applied to all sources | VS needs deep-read of actual release notes page. JetBrains needs GH Changelog cross-reference. VS needs dual-source (MS Learn patches + GH Changelog features). | Update Phase 1B skill with per-source extraction strategies. |
| L28 | The pipeline does NOT cut items. It adds them. 100% discovery survival across Aug+Jun+May. Dec had 2 cuts out of 42. | Cross-cycle meta-analysis: 33/33 items survived in Aug+Jun, 40/42 in Dec. The real editorial value is in ADDITIONS, BUNDLING, and EXPANSION. | Reframe Phase 1C as "enrichment + consolidation" not just consolidation. Phase 3 curation's job is bundling + flagship detection + gap-filling, not selection. |
| L29 | Visual Studio is never discovered by Phase 1A/1B — it's always added during curation | Aug, Jun, May: VS items came from devblogs during curation, never from discoveries. Only Dec (with per-source interims) had VS in discoveries. | Add devblogs.microsoft.com/visualstudio and devblogs.microsoft.com/devops to Phase 1A source coverage. |
| L30 | Azure ecosystem content is a consistent curation-add across every cycle | Azure DevOps migration (Jun), Azure MCP Server (Jun), Copilot for Azure GA (Jun). Never in discoveries. | Add Azure DevOps blog and Microsoft Learn as Phase 1A sources. |
| L31 | Enablement resources (playbooks, training, Copilot Fridays) are added during curation every cycle | Aug: 7 resource items added. Jun: course links. May: trust article. | If new enablement resources exist for the date range, add them in Phase 3. Do not pad with stale content (Q4=B). |
| L32 | Hypothesis-first V2 generation with automated scoring achieves 50/50 in 1 rework cycle | V1 scored 31/50 on V2 rubric. V2 scored 50/50 after fixing: (1) consumer plan mention, (2) line count +3 lines, (3) rubric false positives | Always define hypotheses and scoring rubric BEFORE generating content. The rubric catches issues the writer misses. |
| L33 | "Copilot Pro+" triggers the consumer plan mention validator even in enterprise context | validate_newsletter.sh regex catches "Copilot Pro+" regardless of context. V2 fixed by rewording to "Copilot Enterprise subscriptions" | When mentioning subscription tiers, use only "Copilot Enterprise" or "enterprise Copilot subscriptions" — never reference consumer plan names. |
| L34 | VS Code release notes are the richest single source for enterprise-relevant Copilot features | v1.109 yielded 7 enterprise-relevant features: Agent Skills GA, Claude Agent, Copilot Memory, parallel subagents, terminal sandboxing, org-wide instructions, MCP Apps. V1 had 0 because it only read the changelog entry. | Always deep-read the full VS Code release notes page for the current month. The changelog entry is a summary; the release notes page has 10-20x more detail. |
| L35 | Bundling related items into themed bullets reduces noise while increasing information density | Enterprise Governance Roundup consolidated 3 separate items. Deprecations and migration notices consolidate into a single Enterprise & Security bullet (no standalone Migration Notices section). | Apply bundling rules from editorial-intelligence.md: governance cluster -> single roundup, deprecations/migrations -> single Enterprise & Security bullet, model availability -> single bullet. |
| L36 | score-selection.sh had an unbound variable bug for header extraction | Variables skill_headers and bench_headers were referenced before assignment. Script ran without crashing but reported 0% structure overlap. | Always test scoring scripts on known inputs before using to grade work. The bug was invisible because bash set -euo pipefail treats unbound vars as errors but the scoring continued with defaults. |
| L37 | Consolidated curated sections score poorly against granular benchmarks | Dec 2025 curated output had 23 bullets (consolidated) vs 37 in benchmark (granular). Selection score was 15/25 before expansion. After restructuring to match benchmark's section layout: 21/25. | The curation skill's bundling rules (from editorial intelligence) produce a different structure than the original pre-intelligence benchmark. Benchmarks must be updated to reflect current editorial style, or scoring must account for intentional structural divergence. |
| L38 | Phase gate orchestrator catches missing prerequisites without crashing | run_newsletter.sh correctly blocked at Phase 3 when Phases 1A-1C outputs were missing, printed clear instructions, and exited with code 1. | File-existence gates with size checks (>10 bytes) are sufficient for pipeline sequencing. The orchestrator's value is in gating and validation, not content generation. |
| L39 | Three source gaps (VS devblogs, Azure devblogs, news-insights) now closed in SOURCES.yaml | L19, L29, L30 all identified these gaps. Adding them to SOURCES.yaml (69->72 sources) ensures the url-manifest skill can generate candidates for them. | Source coverage gaps should be fixed in SOURCES.yaml immediately when identified, not deferred to future sprints. |
| L40 | `set -euo pipefail` in scoring scripts causes silent hangs when glob patterns match nothing | `ls output/*_v[12].md` returns non-zero exit in strict mode, killing the subshell before wc -l runs. Script appears to hang. | Use `set -uo pipefail` (no -e) in scoring scripts. Wrap globs in `(cmd || true)` before piping. |
| L41 | Scoring scripts should avoid `make` invocations and call scripts directly | `make validate-newsletter` inside a scoring script causes pipe buffering issues. Direct `bash script.sh` calls are more reliable. | In scoring rubric scripts, call the underlying script directly, not via make targets. |
| L42 | The editorial review loop is a skill, not a script | L23 called for automating the review loop. The implementation is a SKILL.md (106 lines) with a correction format spec (81 lines), not a bash script. The skill tells an agent what to do; the agent does the LLM work. | Automation in an LLM-first system means writing skills (instructions for agents), not scripts (instructions for computers). Scripts handle gating and validation; skills handle content generation. |
| L43 | Structure validation must be updated when deleting directories | `make validate-structure` checked for prompts-legacy/ after it was deleted per deletion discipline, causing a false failure. | When deleting a directory, also update any validation scripts/Makefile targets that check for its existence. |
| L44 | Periodic repo sync catches drift between documentation and implementation | README.md still described a "KB repo", HANDOFF.md was stale, BUILD_PLAN.md was not marked complete, run_newsletter.sh had a macOS-only date command. | Schedule sync bundles after every major sprint. Documentation drift compounds if left unchecked. |
| L45 | Pre-agentic newsletters (Aug 2025 and earlier) legitimately fail current validation rules | August.md has 10 em dashes and 26 wikilinks. These were standard practice before the agentic era (Jun 2025+). | Only test agentic-era newsletters (Dec 2025+) as known-good inputs. Pre-agentic newsletters are useful as benchmark data but not as validation targets. |
| L46 | Test scripts must not use set -e when wrapping commands that may return non-zero legitimately | Archive test exited 127 when a glob matched nothing inside set -euo pipefail. The test runner captured this as a suite failure. | Use set -uo pipefail (no -e) in test scripts. Wrap expected-failure commands in explicit checks. |
| L47 | Multi-cycle benchmark regression reveals era-dependent quality gaps | Jun 2025 scores 9/25 vs Dec 19/25 and Aug 21/25. Jun used a pre-agentic format with different section structure. | Set cycle-appropriate thresholds (lower for older cycles) rather than a single universal bar. The regression test catches real structural evolution, not bugs. |
| L48 | Intelligence sync must be verified by script, not assumed by convention | Weight drift between editorial-intelligence.md and selection-criteria.md was invisible until check_intelligence_sync.sh automated the comparison. 10 weights, 7 surfaces. | Run `make check-intel-sync` after any intelligence file change. Add to CI. |
| L49 | All 7 editorial blind spots (G1-G7) are now encoded as rules | G1 (evergreen resources), G2 (new product sections), G3 (legal expansion), G4 (IDE deep-read), G5 (cross-category bundling), G6 (context-dependent exclusion), G7 (same-type groups) all verified by grep. | The gap encoding test (test_intelligence_effectiveness.sh) is the ground truth for intelligence coverage. |
| L50 | Controlled re-runs from archived intermediates validate intelligence changes without live web fetches | Feb re-run from archived discoveries+events scored 44/50 pre-corrections, 49/50 post-corrections. No web fetches needed for Phases 3-5. | Use archived intermediates for rapid hypothesis testing. Full pipeline (Phases 1A-1B) only needed when testing source discovery changes. |
| L51 | The 5-trigger lead detection model covers all observed leads across 14 newsletters | Retest confirms: competitive positioning, governance clustering, blockbuster launch, hot-topic urgency, and strategic positioning account for all 6 lead sections found. | The 5-trigger model is sufficient. No new trigger types observed in 14 newsletters spanning 2024-2026. |
| L52 | Phase 5 editorial corrections add 5+ points to V2 rubric scores | Pre-corrections: 44/50. Post-corrections: 49/50. Corrections added Agent HQ 3P (from news-insights blog) and VS Code deep features (7 granular items). | The editorial review loop is the highest-leverage quality step. Agent HQ 3P was invisible to the pipeline but was the most impactful item of the month. |
| L53 | EH-9 (weighted vs equal-weight model comparison) requires A/B testing infrastructure not yet built | Cannot test with existing data: Aug fixture is the published newsletter (100% self-overlap). Need to run curation skill twice with different weight configurations. | Build A/B testing support for weight model comparison in a future sprint. |
| L54 | Calibrated weights and equal weights select the same items; weights drive structure, not selection | A/B test on Feb discoveries: both models pick 17/19 published items. Calibrated adds lead section + governance bundling; equal keeps items flat. score-selection.sh structure overlap: calibrated 20% vs equal 10%. | The weight model's value is in triggering lead sections and governance bundling, not in choosing different items. Keep weights for structural decisions; don't expect them to change item selection. |
| L55 | GitHub Discussion edit history is accessible via GraphQL userContentEdits and contains full body snapshots at each revision | 14 discussions, 145 snapshots, 132 computable diffs extracted. Each "diff" field is actually a full body snapshot, not a traditional diff. Python difflib computes real diffs. | Use extract_discussion_edits.py to mine polishing data. The API returns newest-first; reverse for chronological order. |
| L56 | 34% of manual polishing edits are eliminable by automated structural rules | 13 recurring patterns identified from 132 diffs: heading space (15%), bullet normalization (23%), label format (31%), product names (23%), link refinement (54%), late additions (69%), wording precision (77%). Tier 1 structural fixes alone would prevent ~45 of 132 diffs. | Apply Tier 1 fixes automatically in Phase 4.5 (newsletter-polishing skill). The remaining 66% requires editorial judgment or stronger Phase 3/4 prompts. |
| L57 | The richest polishing signal is "write the introduction last" | 38% of newsletters rewrite the intro highlights paragraph at least once because it drifts from the body content during editing. The intro should be generated AFTER all body sections are finalized. | Phase 4 assembly should generate the introduction as the final step, using actual section content as input for the highlights summary. |
| L70 | Deprecation notes are easier to consume when centralized under Enterprise & Security | Feb 2026 had duplicated deprecation links across sections and a standalone Migration Notices footer. Consolidating removed duplication and kept the ending clean. | Add a dedicated skill (`deprecation-consolidation`) and validator rule: no `# Migration Notices`; use one bundled `Deprecations and Migration Notices` bullet under Enterprise & Security. |
| L71 | Copilot CLI primary source is GitHub Releases, not the blog changelog | Feb 2026: CLI bullet only referenced blog changelog posts, missing ~80% of features shipped in daily releases (v0.0.399–v0.0.408). Blog posts are occasional summaries that lag behind by days/weeks. The releases page had background agents, plugin ecosystem, autopilot mode, MCP config, `/instructions`, `/diff` — none of which appeared in the newsletter until manual review of the releases page. | Added CLI-specific extraction strategy to content-retrieval SKILL.md. Created `reference/source-intelligence/copilot-cli.md`. Updated SOURCES.yaml notes. The releases page (`github.com/github/copilot-cli/releases`) is the discovery source; blog posts are supplementary links for narrative context. |
| L72 | GA labels must have explicit source evidence; omit when ambiguous instead of defaulting to PREVIEW | Feb 2026: BYOK was labeled `GA` but changelog says "public preview". OpenCode was labeled `GA` but changelog has no status label at all. Parallel Subagents was labeled `GA` but VS Code release notes have no explicit label. The pipeline's prior rule was "when in doubt, default to PREVIEW" which was still too aggressive. | Updated content-retrieval SKILL.md status verification rule: labels require exact source text ("generally available", "public preview", etc.). If no explicit qualifier, omit the label entirely. Added common over-claim examples (BYOK, OpenCode) as cautionary patterns. |

## Design Patterns (Correlate with Quality)

| Pattern | Why it works |
|---|---|
| File-driven config | Agents in --no-ask-user mode can't ask; file reads always work |
| Explicit stop rules per prompt | Prevents runaway output |
| Named agent files with model: field | Without this, fleet sub-agents default to cheapest model |
| Disk-based phase gates | Prevents phantom completion claims |
| RUN_ID-scoped artifacts | Enables comparison across runs |

## Anti-Patterns (Avoid)

| Anti-pattern | What goes wrong |
|---|---|
| Trusting self-reports over disk verification | Agents claim "quality testing complete" with 0 files on disk |
| Multiple fixes per iteration | Can't isolate which fix worked |
| No stop rules | Agent analyzes forever, never ships |
| Scattered logs | Can't compare runs or trace failures |
| L58 | Superlative platform claims ("any agent, any model") are factually inaccurate and erode trust | Feb 2026 V1: "bring any agent, use any model" was overclaiming; corrected to "more agents, more models, more surfaces" with value prop | Add superlative audit to polishing Tier 3 (rule 17), validation script, and content-format-spec. The value prop is "one set of terms, one payment, one platform" -- not a capability claim |
| L59 | Link labels must match URL patterns, not be generic. "Announcement" for changelog URLs and "CPO Blog" (meaningless to readers) are common errors | Feb 2026: all changelog links labeled [Announcement], blog posts labeled [CPO Blog]. Customers dont know what CPO means | Add URL-to-label canonical mapping to content-format-spec. Validate in polishing Tier 2 (rule 12b) and validation script |
| L60 | Features get misattributed to the wrong product surface when announced alongside IDE updates | Feb 2026: Agents Tab (github.com) placed under VS Code section; ACP (CLI) also misplaced under VS Code | Add surface attribution rules to content-retrieval and polishing Tier 3 (rule 18). Check actual URL/docs to confirm surface |
| L61 | IDE Parity needs its own section when non-VS-Code IDEs have rich releases | Feb 2026: JetBrains had 4 releases (v1.5.60-1.5.64) with significant features, Xcode had MCP registry support -- all collapsed into one-liner | Add escalation trigger to ide-parity-rules: >=3 JetBrains releases or >=5 total IDE items triggers own section |
| L62 | Date range must overlap with previous newsletter to avoid content gaps | Feb 2026: Dec newsletter covered through Dec 3, but Jan start was Jan 1 -- Dec 4-31 fell into a gap | Add Step 0 (date range overlap check) to url-manifest skill. Check archive/ for previous newsletter end date |
| L63 | Categories from recent newsletters that have zero items must be explicitly audited, not silently omitted | Feb 2026: Code Quality had no updates but this was never checked or noted | Add coverage audit step (Step 5.5) to content-consolidation. List all standard categories and note which have zero items |
| L64 | VS Code named months lag actual release dates by 1-2 months; relying on SOURCES.yaml latest_known or named month causes version misses | Feb 2026: v1.108 titled "December 2025" was released Jan 8, 2026 -- within DATE_RANGE but excluded from scope contract, URL manifest, and content retrieval. When challenged, agent fabricated "released Nov 2025" based on named month. Agent confirmed its own wrong answer 3 times. | Scope contract Step 2 MUST fetch code.visualstudio.com/updates index to enumerate ALL VS Code versions by actual release date. url-manifest Step 1 MUST do the same. content-retrieval must check scope contract version list before processing. SOURCES.yaml now has `previous_versions_in_cycle` field. Gate: >=2 VS Code versions for any 30+ day range |
| L65 | The human's monthly notes file (brain dump of links) contributes 10-18% unique content that the pipeline cannot discover | Analysis of 10 benchmark cycles: community resources, team member gists, VS Code extensions, conference videos, and editorial priority signals all come exclusively from the notes file. Pipeline handles commodity changelog scanning; notes inject the "personally curated" layer. | Built curator-notes skill (Phase 1.5). Processes notes file into extracted items + editorial signals. Named feature hints get 100% survival. Internal links get translated to editorial signals. Integrated into pipeline between Phase 1C and Phase 3. |
| L66 | VS Code weekly releases make version-centric extraction and presentation unworkable; multi-release synthesis requires feature-centric diff-based extraction and cross-IDE alignment matrix | Feb 2026: 2 monthly versions produced a "latest version summary + afterthought bullet" anti-pattern. With weekly cadence (4-5 versions per 30-day period), this would produce 4-5 version summary bullets or massive redundancy. December 2025 gold standard (v1.105+v1.106) proved features should be merged by theme, not listed by version. IDE parity section was missing Eclipse, sparse on labels, and used version-centric JetBrains format. | 3-layer fix: (1) Phase 1B diff-based extraction (read newest VS Code page first, diff earlier for status changes), (2) Phase 3 cross-IDE feature alignment matrix with mandatory per-feature labeling and all-4-IDE inclusion, (3) Phase 4 assembly quality checks for IDE parity completeness and no-version-numbers-in-body rule. Updated scope contract gate from >=2 to >=4 versions for 30+ day ranges. |
| L67 | Three formatting bugs stem from format-spec examples and missing rules: (a) model consolidation example used prose grouping instead of per-model backtick labels, (b) no link format rule for 5+ links caused sub-bullet lists, (c) ambiguous status on model extensions caused cross-section GA/PREVIEW conflicts | Feb 2026: Model bullet said "Now GA: X, Y, Z" instead of "X (`GA`), Y (`GA`)". CLI bullet had 9 sub-bullet links. Gemini 3 Flash was PREVIEW in model bullet but GA in IDE parity bullet. | 3 fixes: (1) Per-model backtick labeling rule + fixed example in format-spec, (2) Threshold-based link format: 1-3 inline, 4-6 inline-wrap, 7+ inline-feature-linking where feature names in prose ARE the links, (3) Default-to-PREVIEW when status ambiguous + cross-section consistency audit in polishing Tier 3 rule 19. |
| L68 | Eight editorial patterns from Feb 2026 human corrections reveal systemic gaps in section structure, link quality, item ordering, and consolidation | Feb 2026: (a) Link labels included status text from changelog titles, (b) github.com Agents Tab misplaced in VS Code section (L60 not enforced at assembly), (c) no docs/implementation links alongside changelog links, (d) non-Copilot items (GHEC DR, GHES) in Copilot at Scale, (e) same-domain items not consolidated (2 secret scanning bullets, code scanning + taskflow separate), (f) no revenue-first ordering, (g) no commercial context on cost-relevant features, (h) GHAS trials buried instead of led with | 8 rules added: (1) Link labels never include status text, (2) Surface attribution enforced at assembly, (3) Every bullet gets docs/implementation link, (4) Revenue-positive items sort first, (5) Non-Copilot enterprise items in Enterprise & Security not Copilot at Scale, (6) Same-domain consolidation mandatory, (7) Commercial context on cost-relevant features, (8) Section naming reflects content scope |
| L69 | Microsoft Reactor is a rich event source (~28 GitHub-tagged events at any time) that the pipeline was not scanning; events require filtering because most are not enterprise-relevant | Feb 2026: Reactor search for "github" returns 28 events. Human selected 14. Excluded: non-English (Spanish series), competition/gaming formats (Agents League Battle), generic AI tutorials without GitHub integration (LangChain4j). Included: Agentic DevOps Live series (modernization, security, SRE), VS Code Live releases, AI Dev Days hackathon, Python + Agents with MCP. Key series to track: Agentic DevOps Live (S-1625), VS Code Live (S-1521). | Added Reactor as mandatory event source with include/exclude filter rules. Updated events-extraction SKILL.md Step 1 (scan Reactor) and Step 1.5 (filtering). Added source intelligence to events-formatting.md. Previous newsletters used Reactor occasionally (May/Jun 2025 SF AI Show + Tell) but never systematically. |
